{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#JupyterHub\" data-toc-modified-id=\"JupyterHub-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>JupyterHub</a></div><div class=\"lev2 toc-item\"><a href=\"#Installing-Jupyter-Notebook\" data-toc-modified-id=\"Installing-Jupyter-Notebook-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Installing Jupyter Notebook</a></div><div class=\"lev3 toc-item\"><a href=\"#Install-Anaconda-for-Python3\" data-toc-modified-id=\"Install-Anaconda-for-Python3-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Install Anaconda for Python3</a></div><div class=\"lev3 toc-item\"><a href=\"#Install-Anaconda\" data-toc-modified-id=\"Install-Anaconda-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Install Anaconda</a></div><div class=\"lev2 toc-item\"><a href=\"#Configure-Jupyter-Notebook\" data-toc-modified-id=\"Configure-Jupyter-Notebook-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Configure Jupyter Notebook</a></div><div class=\"lev2 toc-item\"><a href=\"#Test-the-notebook-server\" data-toc-modified-id=\"Test-the-notebook-server-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Test the notebook server</a></div><div class=\"lev2 toc-item\"><a href=\"#Adding-the-kernels-to-the-notebook-server\" data-toc-modified-id=\"Adding-the-kernels-to-the-notebook-server-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Adding the kernels to the notebook server</a></div><div class=\"lev3 toc-item\"><a href=\"#Add-PySpark-kernel\" data-toc-modified-id=\"Add-PySpark-kernel-141\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Add PySpark kernel</a></div><div class=\"lev4 toc-item\"><a href=\"#Check-Python-version\" data-toc-modified-id=\"Check-Python-version-1411\"><span class=\"toc-item-num\">1.4.1.1&nbsp;&nbsp;</span>Check Python version</a></div><div class=\"lev4 toc-item\"><a href=\"#Install-IPython-for-Python2\" data-toc-modified-id=\"Install-IPython-for-Python2-1412\"><span class=\"toc-item-num\">1.4.1.2&nbsp;&nbsp;</span>Install IPython for Python2</a></div><div class=\"lev4 toc-item\"><a href=\"#Testing-the-PySpark-kernel\" data-toc-modified-id=\"Testing-the-PySpark-kernel-1413\"><span class=\"toc-item-num\">1.4.1.3&nbsp;&nbsp;</span>Testing the PySpark kernel</a></div><div class=\"lev4 toc-item\"><a href=\"#Errors-in-notebook-server\" data-toc-modified-id=\"Errors-in-notebook-server-1414\"><span class=\"toc-item-num\">1.4.1.4&nbsp;&nbsp;</span>Errors in notebook server</a></div><div class=\"lev3 toc-item\"><a href=\"#Add-the-R-kernel-(compile)\" data-toc-modified-id=\"Add-the-R-kernel-(compile)-142\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Add the R kernel (compile)</a></div><div class=\"lev4 toc-item\"><a href=\"#Testing-R\" data-toc-modified-id=\"Testing-R-1421\"><span class=\"toc-item-num\">1.4.2.1&nbsp;&nbsp;</span>Testing R</a></div><div class=\"lev4 toc-item\"><a href=\"#Test-SparkR\" data-toc-modified-id=\"Test-SparkR-1422\"><span class=\"toc-item-num\">1.4.2.2&nbsp;&nbsp;</span>Test SparkR</a></div><div class=\"lev3 toc-item\"><a href=\"#Add-the-Spark-Scala-kernel-(through-Apache-Toree)\" data-toc-modified-id=\"Add-the-Spark-Scala-kernel-(through-Apache-Toree)-143\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>Add the Spark-Scala kernel (through Apache Toree)</a></div><div class=\"lev4 toc-item\"><a href=\"#Test-Spark-Scala\" data-toc-modified-id=\"Test-Spark-Scala-1431\"><span class=\"toc-item-num\">1.4.3.1&nbsp;&nbsp;</span>Test Spark-Scala</a></div><div class=\"lev2 toc-item\"><a href=\"#Installing-JupyterHub\" data-toc-modified-id=\"Installing-JupyterHub-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Installing JupyterHub</a></div><div class=\"lev3 toc-item\"><a href=\"#Install-NodeJS-and-its-package-manager\" data-toc-modified-id=\"Install-NodeJS-and-its-package-manager-151\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Install NodeJS and its package manager</a></div><div class=\"lev3 toc-item\"><a href=\"#Install-JupyterHub\" data-toc-modified-id=\"Install-JupyterHub-152\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Install JupyterHub</a></div><div class=\"lev3 toc-item\"><a href=\"#Configure-JupyterHub\" data-toc-modified-id=\"Configure-JupyterHub-153\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Configure JupyterHub</a></div><div class=\"lev3 toc-item\"><a href=\"#Test-JupyterHub\" data-toc-modified-id=\"Test-JupyterHub-154\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Test JupyterHub</a></div><div class=\"lev3 toc-item\"><a href=\"#Locking-down-the-environment\" data-toc-modified-id=\"Locking-down-the-environment-155\"><span class=\"toc-item-num\">1.5.5&nbsp;&nbsp;</span>Locking down the environment</a></div><div class=\"lev4 toc-item\"><a href=\"#Running-JupyterHub-without-root\" data-toc-modified-id=\"Running-JupyterHub-without-root-1551\"><span class=\"toc-item-num\">1.5.5.1&nbsp;&nbsp;</span>Running JupyterHub without root</a></div><div class=\"lev4 toc-item\"><a href=\"#Setting-up-SSL-encryption\" data-toc-modified-id=\"Setting-up-SSL-encryption-1552\"><span class=\"toc-item-num\">1.5.5.2&nbsp;&nbsp;</span>Setting up SSL encryption</a></div><div class=\"lev4 toc-item\"><a href=\"#Create-a-whitelist-of-allowed-users\" data-toc-modified-id=\"Create-a-whitelist-of-allowed-users-1553\"><span class=\"toc-item-num\">1.5.5.3&nbsp;&nbsp;</span>Create a whitelist of allowed users</a></div><div class=\"lev4 toc-item\"><a href=\"#Create-directories-to-keep-the-notebooks\" data-toc-modified-id=\"Create-directories-to-keep-the-notebooks-1554\"><span class=\"toc-item-num\">1.5.5.4&nbsp;&nbsp;</span>Create directories to keep the notebooks</a></div><div class=\"lev2 toc-item\"><a href=\"#JupyterHub-as-system-service\" data-toc-modified-id=\"JupyterHub-as-system-service-16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>JupyterHub as system service</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterHub\n",
    "Multi-Tenant notebook implementation\n",
    "Implementation steps\n",
    "Version 6, 2016-11-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set the name of the server to install on. We also define the an alias 'remote' to ease writing commands ot the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export SERVER=kac-srv-001.kac.sblokalnet\n",
    "export LUSER=abr-sadm\n",
    "alias remote=\"ssh -t $LUSER@$SERVER export PATH=/opt/anaconda3/bin:\\$PATH \\&\\& \"\n",
    "alias remotesudo=\"remote cd /tmp \\&\\& sudo env PATH=/opt/anaconda3/bin:\\$PATH \"\n",
    "alias start_notebook='remote \\(yes \\| jupyter notebook --ip=\\$\\(hostname -f \\)\\)'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Jupyter Notebook\n",
    "\n",
    "\n",
    "First we will install and test the Jupyter notebook as this is the basis for the user interface presented to the analysts. Both Jupyter and JupyterHub are best installed through Anaconda, a Python distribution that includes most libraries needed by people tasked with the analysis of data.\n",
    "\n",
    "### Install Anaconda for Python3\n",
    "\n",
    "IBM Open Platform is based on Python2. JupyterHub however requires Python3, so we must first start with Anaconda for Python3.\n",
    "Download Anaconda\n",
    "Download Anaconda 4.x for Python3 from: https://www.continuum.io/downloads \n",
    "\n",
    "Once downloaded, upload the shell script (Anaconda3….sh) to the BigInsights node that runs the Spark driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Anaconda\n",
    "Run the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote rm -rf /tmp/Anaconda3-4.2.0-Linux-x86_64.sh\n",
    "remote wget https://repo.continuum.io/archive/Anaconda3-4.2.0-Linux-x86_64.sh -O /tmp/Anaconda3-4.2.0-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start the installation; this can take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote chmod a+x /tmp/Anaconda3-4.2.0-Linux-x86_64.sh\n",
    "remotesudo bash /tmp/Anaconda3-4.2.0-Linux-x86_64.sh -b -p /opt/anaconda3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installation now starts; this can take a couple of minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Jupyter Notebook\n",
    "As we’re planning to use a single configuration across notebook users, the notebook configuration will be created at a central location, /etc/jupyter.\n",
    "Execute the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remotesudo mkdir -p /etc/jupyter\n",
    "remotesudo jupyter notebook -y --generate-config --config=/etc/jupyter/jupyter_notebook_config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the configuration so that Jupyter will not try and start a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remotesudo \"sed -i 's/^[#]\\?c\\.NotebookApp\\.open_browser.*/c.NotebookApp.open_browser=False'/ /etc/jupyter/jupyter_notebook_config.py\"\n",
    "remote grep open_browser /etc/jupyter/jupyter_notebook_config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the parameter \"c.NotebookApp.open_browser\" to \"False\" and save the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the notebook server\n",
    "Now that the Jupyter notebook is configured, run a basic test against it by starting it and opening a notebook. For basic Python testing, you can start the notebook server as user root, but with Spark notebooks you will need access to the HDFS directory structure.\n",
    "\n",
    "Start the notebook server as your normal user by entering the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some information and warning messages will be issued. You can ignore the warning messages; they do not hinder the running of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, start a browser and navigate to the Jupyter notebook using address:\n",
    "http://jupyter-system-ip-address:8888 \n",
    "Create a new Python notebook and enter a Python statement, for example “print(1+1)”. Then execute it using Shift-Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](jupyterInitial.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’ve tested that the notebook server works, shut it down by interrupting the above command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the kernels to the notebook server\n",
    "By default, only a Python3 kernel is available from the notebook server. In this section we will add the PySpark, Scala and R kernels to the notebook configuration so they can be used by the analysts.\n",
    "Even though it is possible to configure kernels per user, we define the kernels in a central location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add PySpark kernel\n",
    "Before adding the PySpark kernel to the Jupyter notebook, we first need to ensure that the ipykernel is available for Python2, as this is the basis for Spark 1.x.y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Python version\n",
    "Please ensure that the python command executes Python 2.7. You may have to remove /opt/anaconda3/bin from the path and/or add the /opt/anaconda2/bin at the beginning of the path.\n",
    "Run the following command to determine the Python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote python2 -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be like the following:\n",
    "\n",
    "    Python 2.7.12 :: Anaconda 4.2.0 (64-bit)\n",
    "or\n",
    "\n",
    "    Python 2.7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, record the location of the command. You will need this later when you specify the path in the kernel.json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote which -a python2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the python command was found in /bin. Another location could be /usr/bin or /opt/anaconda2/bin (if you have installed it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Install IPython for Python2\n",
    "Dependent on whether you have installed Anaconda2 or you’re using a pre-installed Python2 package, you may have to install pip and the IPython kernel. Run the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remotesudo yum install -y python-pip\n",
    "remotesudo pip install ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ignore the warning messages if pip and the IPython kernel are already installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands to add the PySpark kernel to the notebook configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remotesudo mkdir -p /usr/share/jupyter/kernels/pyspark\n",
    "remotesudo touch /usr/share/jupyter/kernels/pyspark/kernel.json\n",
    "python << EOJ | remotesudo tee /usr/share/jupyter/kernels/pyspark/kernel.json\n",
    "print(\"\"\"\n",
    "{\n",
    "    \"display_name\": \"PySpark\",\n",
    "    \"language\": \"python\",\n",
    "    \"argv\": [ \"/usr/bin/python2\", \"-m\", \"ipykernel\", \"-f\", \"{connection_file}\" ],\n",
    "    \"env\": {\n",
    "        \"PATH\": \"/bin:/sbin:/usr/sbin:/usr/bin:/usr/ibmpacks/current/bigsql/bigsql/bin\",\n",
    "        \"SPARK_HOME\": \"/usr/iop/current/spark-client\",\n",
    "        \"PYSPARK_PYTHON\": \"/usr/bin/python2\",\n",
    "        \"PYTHONPATH\": \"/usr/iop/current/spark-client/python/:/usr/iop/current/spark-client/python/lib/py4j-0.9-src.zip\",\n",
    "        \"PYTHONSTARTUP\": \"/usr/iop/current/spark-client/python/pyspark/shell.py\",\n",
    "        \"PYSPARK_SUBMIT_ARGS\": \"--master yarn-client pyspark-shell\"\n",
    "    }\n",
    "}\"\"\")\n",
    "EOJ\n",
    "#remote cat /usr/share/jupyter/kernels/pyspark/kernel.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a close look at the various properties to ensure they match your environment.\n",
    "* \"argv\": \"/bin/python\" The full path of the python command as determined in the previous section\n",
    "* \"PATH\": \"/bin: …\" Ensure that the directory holding the python command is first in the path\n",
    "* \"PYSPARK_PYTHON\": \"/bin/python\" The full path of the python command as determined in the previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\tMake sure you don’t copy any special characters such as tabs or formatting into the JSON file, otherwise it will fail to be parsed.\n",
    "\n",
    "**Path:**\tThe PATH is explicitly set in the kernel.json file to ensure that the PySpark shell scripts are executed using Python2, instead of Python3. Failing to do so results in syntax errors in the notebook server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once finished, check that the kernel is now available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should render the following output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Available kernels:\n",
    "      python3    /opt/anaconda3/lib/python3.5/site-packages/ipykernel/resources\n",
    "      pyspark    /usr/share/jupyter/kernels/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the PySpark kernel\n",
    "Once the PySpark kernel has been added, start the notebook server using user spark by entering the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a basic expression such as “print(sc.version)”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](jupyterSpark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another test you can do is to write a file into Hadoop and subsequently read it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    l=range(1000000)\n",
    "    rdd=sc.parallelize(l).map(lambda x:x+1)\n",
    "    rdd.saveAsTextFile('/tmp/output.txt')\n",
    "    readFile=sc.textFile('/tmp/output.txt')\n",
    "    readFile.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should return a value of 1000000.\n",
    "\n",
    "Once finished, save or discard the notebook and close the notebook using File -> Close and Halt. This will also stop the PySpark session.\n",
    "\n",
    "In the terminal that is running the notebook server, you will see a lot of messages when Spark is started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors in notebook server\n",
    "##### Endless messages in notebook server\n",
    "If you start PySpark or any other notebook that is dependent on Spark and you see endless messages in the notebook server:\n",
    "\n",
    "    16/10/15 02:27:38 INFO Client: Application report for application_1476515684661_0002 (state: ACCEPTED)\n",
    "    16/10/15 02:27:39 INFO Client: Application report for application_1476515684661_0002 (state: ACCEPTED)\n",
    "    16/10/15 02:27:40 INFO Client: Application report for application_1476515684661_0002 (state: ACCEPTED)\n",
    "This typically means that YARN was not started or not properly running. Check your Ambari console to ensure YARN and its node managers are up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Errors in the notebook server – Cannot connect to port 8050\n",
    "If errors such as the following appear in the terminal where you have started the Jupyter notebook server, Spark or Hive is probably not started.\n",
    "\n",
    "    16/10/14 04:43:24 INFO Client: Retrying connect to server: biginsights-sn.demos.demoibm.com/10.137.41.71:8050. Already tried 22 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS) \n",
    "In that case you should check the Ambari console to see that the services are running. Also you can test that PySpark is working properly by running the following command as the Spark user and check that you can enter PySpark commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python errors in Jupyter console – missing parentheses\n",
    "If PySpark (or one of the other Spark-related kernels) is started with Python3, you will see errors in the notebook server, even though the notebook itself may seem to work properly.\n",
    "\n",
    "    16/10/04 12:17:13 WARN ScriptBasedMapping: Exception running /etc/hadoop/conf/topology_script.py 172.16.7.130 \n",
    "    ExitCodeException exitCode=1:   File \"/etc/hadoop/conf/topology_script.py\", line 63\n",
    "        print rack\n",
    "                 ^\n",
    "    SyntaxError: Missing parentheses in call to 'print'\n",
    "In the example PySpark kernel.json, we are explicitly setting the PATH environment variable to ensure /bin/python is at the beginning of the path. Make sure that /bin/python starts Python2.\n",
    "\n",
    "    [spark@biginsights-sn ~]$ /bin/python\n",
    "    Python 2.7.5 (default, Oct 11 2015, 17:47:16) \n",
    "    [GCC 4.8.3 20140911 (Red Hat 4.8.3-9)] on linux2\n",
    "    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "    >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Add the R kernel (compile)\n",
    "As the R kernel is not part of the CRAN repos it should be compiled from sources. Start by getting the following additional packages (if they are not already part of your OS installation). Run the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remotesudo yum install -y R\n",
    "remotesudo yum install -y openssl-devel openssl libcurl-devel libssh2-devel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create links to libssl.so.1.0.0 and libcrypto.so.1.0.0 under /usr/lib64 to avoid errors like \"libssl.so.10: cannot open shared object file\" during compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remotesudo ln -s /opt/anaconda3/lib/libssl.so.1.0.0 /usr/lib64/libssl.so.1.0.0\n",
    "remotesudo ln -s /opt/anaconda3/lib/libcrypto.so.1.0.0 /usr/lib64/libcrypto.so.1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start R using root and install the git2r, devtools, repr, IRdisplay, crayon and pbdZMQ packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote sudo -i R --vanilla << EOR\n",
    "install.packages(c('git2r','devtools','repr','IRdisplay','crayon','pbdZMQ'),repos=\"http://cran.rstudio.com/\")\n",
    "EOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete you can use the devtools package to get and compile IRkernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote sudo -i R --vanilla << EOR\n",
    "devtools::install_github('IRkernel/IRkernel')\n",
    "EOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are planning to use SparkR now it's the time to get and install the SparkR package (make sure to use the correct package version for your version of Spark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote sudo -i R --vanilla << EOR\n",
    "devtools::install_github('apache/spark@v1.6.1', subdir='R/pkg')\n",
    "EOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the kernel specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote sudo mkdir -p /usr/share/jupyter/kernels/r\n",
    "python << EOJ | remotesudo tee  /usr/share/jupyter/kernels/r/kernel.json\n",
    "print(\"\"\"\n",
    "{\n",
    " \"argv\": [\"R\", \"--slave\", \"-e\", \"IRkernel::main()\", \"--args\", \"{connection_file}\"],\n",
    " \"display_name\":\"R and SparkR\",\n",
    " \"language\":\"R\",\n",
    " \"env\": {\n",
    "    \"PATH\": \"/bin:/usr/ibmpacks/current/bigsql/bigsql/bin:/sbin:/usr/sbin:/usr/bin:/opt/anaconda3/bin\"\n",
    " }\n",
    "}\"\"\")\n",
    "EOJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should look something like this:\n",
    "\n",
    "    Available kernels:\n",
    "      python3    /opt/anaconda3/lib/python3.5/site-packages/ipykernel/resources\n",
    "      pyspark    /usr/share/jupyter/kernels/pyspark\n",
    "      r          /usr/share/jupyter/kernels/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start (restart) your notebook server and verify that the new kernels are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing R\n",
    "First test that R is working by opening a “R and SparkR” notebook and entering the following statements.\n",
    "\n",
    "    a <- c('a','b','c')\n",
    "    a\n",
    "The result should be:\n",
    "\n",
    "![](jupyterR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test SparkR\n",
    "To test SparkR, we will put some test data into your users HFDS home directory (/user/abr). We will parse CSV data and therefore must download and install a parser to be used from the notebook. Run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote sudo wget http://central.maven.org/maven2/com/databricks/spark-csv_2.11/1.5.0/spark-csv_2.11-1.5.0.jar -O /usr/iop/current/spark-client/lib/spark-csv_2.11-1.5.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some test data and upload it to the HDFS directory. Run the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remote << EOL\n",
    "wget -q https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv\n",
    "hdfs dfs -put cars.csv .\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can test R and SparkR by parsing the CSV file and loading the data in an R Data Frame. If the CSV data source is not already part of your Spark installation, you'll have to download it and add it to your Spark libraries.\n",
    "Now, run the Jupyter notebook server as yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new notebook using the “R and SparkR” kernel. Put the following code inside (mind the correct spark-csv version) and run it.\n",
    "\n",
    "    Sys.setenv(SPARK_HOME='/usr/iop/current/spark-client')\n",
    "    .libPaths(c(file.path(Sys.getenv('SPARK_HOME'), 'R', 'lib'), .libPaths()))\n",
    "    library(SparkR)\n",
    "    sc <- sparkR.init(master='yarn-client', sparkPackages=\"com.databricks:spark-csv_2.11:1.5.0\")\n",
    "    sqlContext <- sparkRSQL.init(sc)\n",
    "    df <- read.df(sqlContext, \"cars.csv\", source = \"com.databricks.spark.csv\", inferSchema = \"true\", header=\"true\")\n",
    "    head(df) \n",
    "    \n",
    "The result should be the following.\n",
    "![](jupyterSparkR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the Spark-Scala kernel (through Apache Toree)\n",
    "The steps in this section are required to be able to run Spark Scala scripts from the Jupyter notebook. Run the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remotesudo pip install toree\n",
    "remotesudo jupyter toree install --spark_home=/usr/iop/4.2.0.0/spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Spark-Scala\n",
    "Now, run the Jupyter notebook server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new Apache Toree - Scala notebook and enter the following statements:\n",
    "\n",
    "    val NUM_SAMPLES=1000000\n",
    "    val count = sc.parallelize(1 to NUM_SAMPLES).map{i =>\n",
    "      val x = Math.random()\n",
    "      val y = Math.random()\n",
    "      if (x*x + y*y < 1) 1 else 0\n",
    "    }.reduce(_ + _)\n",
    "    println(\"Pi is roughly \" + 4.0 * count / NUM_SAMPLES)\n",
    "You should expect the following result.\n",
    "![](jupyterScala.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Installing JupyterHub\n",
    "When the Jupyter notebook is up and running and basic tests have been performed, it is now time to implement the multi-tenant access to Jupyter, through JupyterHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install NodeJS and its package manager\n",
    "JupyterHub is distributed as a NodeJS web application, so first NodeJS and its package manager must be installed. Execute the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remotesudo yum install -y npm nodejs\n",
    "remotesudo npm install -g configurable-http-proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install JupyterHub\n",
    "Install JupyterHub using pip (part of the Anaconda for Python3 installation). Run the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remotesudo pip install jupyterhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure JupyterHub\n",
    "Once installed, create an initial configuration in the /etc/jupyter directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remotesudo jupyterhub -y True --generate-config --config=/etc/jupyterhub/jupyterhub_config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test JupyterHub\n",
    "To check that JupyterHub works, we will first run it as user **root** without SSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remotesudo jupyterhub -f /etc/jupyterhub/jupyterhub_config.py --no-ssl --ip=\\$\\(hostname -f \\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the users connecting to JupyterHub will be validated by the operating system using PAM authentication. The users we connect with must have a password.\n",
    "\n",
    "For the validation we assume that two non-privileged users exist on the system, **nick** and **frank**. Both users have a home directory on the Linux file system and an HDFS directory they own, respectively /user/nick and /user/frank. The permissions on the HDFS directories have been set to 700 so that only the owner of the directory has full access rights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remote << EOL\n",
    "sudo hdfs dfs -mkdir /user/nick\n",
    "sudo hdfs dfs -mkdir /user/frank\n",
    "sudo hdfs dfs -chown nick:nick /user/nick\n",
    "sudo hdfs dfs -chown frank:frank /user/frank\n",
    "sudo hdfs dfs -chmod 700 /user/nick \n",
    "sudo hdfs dfs -chmod 700 /user/frank\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, connect to JupyterHub through URL `http://<server-name>:8000`\n",
    "\n",
    "![](hubLogin.png)\n",
    "\n",
    "Log on with user **nick** and the specified password. The list of notebooks should be empty and the permissions are applied when connecting to the HDFS.\n",
    "\n",
    "Create a Scala notebook and enter the following statements.\n",
    "\n",
    "    import scala.io.Source\n",
    "    val html = scala.io.Source.fromURL(\"https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv\").mkString\n",
    "    val list = html.split(\"\\n\").filter(_ != \"\")\n",
    "    val rdds = sc.parallelize(list)\n",
    "    rdds.saveAsTextFile(\"/user/nick/cars.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now log on as user **frank**, create a new notebook and try to access the file in directory /user/nick.\n",
    "\n",
    "    val rdd=sc.textFile(\"/user/nick/car.csv\")\n",
    "    rdd.count    \n",
    "    \n",
    "You will get an error as user **frank** is not allowed to access the file in directory /user/nick.    \n",
    "\n",
    "![](permissionError.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locking down the environment\n",
    "Once you have installed JupyterHub and validated its working, it is best to put some security in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running JupyterHub without root\n",
    "To be finalized, not sure if it is needed.\n",
    "https://github.com/jupyterhub/jupyterhub/wiki/Using-sudo-to-run-JupyterHub-without-root-privileges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/jupyter/sudospawner\n",
      "  Cloning https://github.com/jupyter/sudospawner to ./pip-ucixvgap-build\n",
      "Requirement already satisfied (use --upgrade to upgrade): jupyterhub>=0.4 in /opt/anaconda3/lib/python3.5/site-packages (from sudospawner==0.3.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): notebook in /opt/anaconda3/lib/python3.5/site-packages (from sudospawner==0.3.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests in /opt/anaconda3/lib/python3.5/site-packages (from jupyterhub>=0.4->sudospawner==0.3.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): tornado>=4.1 in /opt/anaconda3/lib/python3.5/site-packages (from jupyterhub>=0.4->sudospawner==0.3.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): traitlets>=4.1 in /opt/anaconda3/lib/python3.5/site-packages (from jupyterhub>=0.4->sudospawner==0.3.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): sqlalchemy>=1.0 in /opt/anaconda3/lib/python3.5/site-packages (from jupyterhub>=0.4->sudospawner==0.3.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pamela in /opt/anaconda3/lib/python3.5/site-packages (from jupyterhub>=0.4->sudospawner==0.3.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): jinja2 in /opt/anaconda3/lib/python3.5/site-packages (from jupyterhub>=0.4->sudospawner==0.3.0)\n",
      "Requirement already satisfied (use --upgrade to upgrade): MarkupSafe in /opt/anaconda3/lib/python3.5/site-packages (from jinja2->jupyterhub>=0.4->sudospawner==0.3.0)\n",
      "Installing collected packages: sudospawner\n",
      "  Running setup.py install for sudospawner ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25hSuccessfully installed sudospawner-0.3.0\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Connection to kac-srv-001.kac.sblokalnet closed.\n"
     ]
    }
   ],
   "source": [
    "remotesudo pip install git+https://github.com/jupyter/sudospawner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up SSL encryption\n",
    "Refer to section https://github.com/jupyterhub/jupyterhub/blob/master/docs/source/getting-started.md#ssl-encryption for setting up SSL for JupyterHub. After setting up SSL, you should run JupyterHub without the --no-ssl parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a whitelist of allowed users\n",
    "This is an extra security measure so that only a specific group of users can access JupyterHub. Update the /etc/jupyter/jupyterhub_config.py file and update the c.Authenticator.whitelist parameter to only include those users who can have access.\n",
    "More information can be found here: https://github.com/jupyterhub/jupyterhub/blob/master/docs/source/getting-started.md#authentication-and-users "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create directories to keep the notebooks\n",
    "To keep notebooks secured from access by other users, it is best to create a directory under the user's home directory and specify this directory as the default notebook directory. More information can be found here: https://github.com/jupyterhub/jupyterhub/blob/master/docs/source/getting-started.md#spawners-and-single-user-notebook-servers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JupyterHub as system service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo-terminal will not be allocated because stdin is not a terminal.\n",
      "\n",
      "[Unit]\n",
      "Description=Jupyterhub\n",
      "After=network-online.target\n",
      "\n",
      "[Service]\n",
      "User=rhea\n",
      "\n",
      "#Two ways of specifying the runtime directory\n",
      "RuntimeDirectory=jupyterhub\n",
      "WorkingDirectory=/var/run/jupyterhub\n",
      "\n",
      "#This sets up the logging directory\n",
      "PermissionsStartOnly=true\n",
      "ExecStartPre=/usr/bin/mkdir -p /var/log/jupyterhub\n",
      "ExecStartPre=/usr/bin/touch /var/log/jupyterhub/jupyterhub.log\n",
      "ExecStartPre=/usr/bin/chown rhea /var/log/jupyterhub -R\n",
      "\n",
      "#Set the path to include anaconda3\n",
      "Environment=\"PATH=/opt/anaconda3/bin/:/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin\"\n",
      "\n",
      "\n",
      "ExecStart=/opt/anaconda3/bin/jupyterhub --config=/etc/jupyterhub/jupyterhub_config.py --no-ssl --log-file=/var/log/jupyterhub/jupyterhub.log --ip=\"*\" --JupyterHub.spawner_class=sudospawner.SudoSpawner\n",
      "\n",
      "\n",
      "[Install]\n",
      "WantedBy=multi-user.target\n",
      "\n",
      "Connection to kac-srv-001.kac.sblokalnet closed.\n"
     ]
    }
   ],
   "source": [
    "python << EOS | remotesudo tee  /lib/systemd/system/jupyterhub.service\n",
    "print(\"\"\"\n",
    "[Unit]\n",
    "Description=Jupyterhub\n",
    "After=network-online.target\n",
    "\n",
    "[Service]\n",
    "User=rhea\n",
    "\n",
    "#Two ways of specifying the runtime directory\n",
    "RuntimeDirectory=jupyterhub\n",
    "WorkingDirectory=/var/run/jupyterhub\n",
    "\n",
    "#This sets up the logging directory\n",
    "PermissionsStartOnly=true\n",
    "ExecStartPre=/usr/bin/mkdir -p /var/log/jupyterhub\n",
    "ExecStartPre=/usr/bin/touch /var/log/jupyterhub/jupyterhub.log\n",
    "ExecStartPre=/usr/bin/chown rhea /var/log/jupyterhub -R\n",
    "\n",
    "#Set the path to include anaconda3\n",
    "Environment=\"PATH=/opt/anaconda3/bin/:/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin\"\n",
    "\n",
    "\n",
    "ExecStart=/opt/anaconda3/bin/jupyterhub --config=/etc/jupyterhub/jupyterhub_config.py --no-ssl --log-file=/var/log/jupyterhub/jupyterhub.log --ip=\"*\" --JupyterHub.spawner_class=sudospawner.SudoSpawner\n",
    "\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target\n",
    "\"\"\")\n",
    "EOS\n",
    "remotesudo systemctl daemon-reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to kac-srv-001.kac.sblokalnet closed.\n",
      "Connection to kac-srv-001.kac.sblokalnet closed.\n",
      "\u001b[1;32m●\u001b[0m jupyterhub.service - Jupyterhub\n",
      "   Loaded: loaded (/usr/lib/systemd/system/jupyterhub.service; enabled; vendor preset: disabled)\n",
      "   Active: \u001b[1;32mactive (running)\u001b[0m since man 2016-11-14 16:51:47 CET; 321ms ago\n",
      "  Process: 373 ExecStartPre=/usr/bin/chown rhea /var/log/jupyterhub -R (code=exited, status=0/SUCCESS)\n",
      "  Process: 371 ExecStartPre=/usr/bin/touch /var/log/jupyterhub/jupyterhub.log (code=exited, status=0/SUCCESS)\n",
      "  Process: 368 ExecStartPre=/usr/bin/mkdir -p /var/log/jupyterhub (code=exited, status=0/SUCCESS)\n",
      " Main PID: 375 (jupyterhub)\n",
      "   CGroup: /system.slice/jupyterhub.service\n",
      "           └─375 /opt/anaconda3/bin/python /opt/anaconda3/bin/jupyterhub --config=/etc/jupyterhub/jupyterhub_config.py --no-ssl --log-file=/var/log/jupyterhub/jupyterhub.log --ip=\"*\" --JupyterHub.spawner_class=sudospawner.SudoSpawner\n",
      "\n",
      "nov 14 16:51:47 kac-srv-001.kac.sblokalnet systemd[1]: Starting Jupyterhub...\n",
      "nov 14 16:51:47 kac-srv-001.kac.sblokalnet systemd[1]: Started Jupyterhub.\n",
      "Connection to kac-srv-001.kac.sblokalnet closed.\n"
     ]
    }
   ],
   "source": [
    "remotesudo systemctl stop jupyterhub\n",
    "remotesudo systemctl start jupyterhub\n",
    "remotesudo systemctl status jupyterhub -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m●\u001b[0m jupyterhub.service - Jupyterhub\n",
      "   Loaded: loaded (/usr/lib/systemd/system/jupyterhub.service; disabled; vendor preset: disabled)\n",
      "   Active: \u001b[1;32mactive (running)\u001b[0m since man 2016-11-14 15:39:41 CET; 3s ago\n",
      "  Process: 58765 ExecStartPre=/usr/bin/chown rhea /var/log/jupyterhub -R (code=exited, status=0/SUCCESS)\n",
      "  Process: 58763 ExecStartPre=/usr/bin/touch /var/log/jupyterhub/jupyterhub.log (code=exited, status=0/SUCCESS)\n",
      "  Process: 58760 ExecStartPre=/usr/bin/mkdir -p /var/log/jupyterhub (code=exited, status=0/SUCCESS)\n",
      " Main PID: 58767 (jupyterhub)\n",
      "   CGroup: /system.slice/jupyterhub.service\n",
      "           ├─58767 /opt/anaconda3/bin/python /opt/anaconda3/bin/jupyterhub --config=/etc/jupyterhub/jupyterhub_config.py --no-ssl --log-file=/var/log/jupyterhub/jupyterhub.log --ip=\"*\" --JupyterHub.spawner_class=sudospawner.SudoSpawner\n",
      "           └─58794 node /bin/configurable-http-proxy --ip * --port 8000 --api-ip 127.0.0.1 --api-port 8001 --default-target http://127.0.0.1:8081 --error-target http://127.0.0.1:8081/hub/error\n",
      "\n",
      "nov 14 15:39:42 kac-srv-001.kac.sblokalnet jupyterhub[58767]: [I 2016-11-14 15:39:42.077 JupyterHub app:1231] Hub API listening on http://127.0.0.1:8081/hub/\n",
      "nov 14 15:39:42 kac-srv-001.kac.sblokalnet jupyterhub[58767]: [E 2016-11-14 15:39:42.080 JupyterHub utils:43] Unexpected error connecting to *:8000 [Errno 101] Network is unreachable\n",
      "nov 14 15:39:42 kac-srv-001.kac.sblokalnet jupyterhub[58767]: [W 2016-11-14 15:39:42.082 JupyterHub app:959] Running JupyterHub without SSL. There better be SSL termination happening somewhere else...\n",
      "nov 14 15:39:42 kac-srv-001.kac.sblokalnet jupyterhub[58767]: [I 2016-11-14 15:39:42.082 JupyterHub app:968] Starting proxy @ http://*:8000/\n",
      "nov 14 15:39:42 kac-srv-001.kac.sblokalnet jupyterhub[58767]: [E 2016-11-14 15:39:42.084 JupyterHub utils:43] Unexpected error connecting to *:8000 [Errno 101] Network is unreachable\n",
      "nov 14 15:39:42 kac-srv-001.kac.sblokalnet jupyterhub[58767]: [E 2016-11-14 15:39:42.186 JupyterHub utils:43] Unexpected error connecting to *:8000 [Errno 101] Network is unreachable\n",
      "nov 14 15:39:42 kac-srv-001.kac.sblokalnet jupyterhub[58767]: 15:39:42.250 - warn: [ConfigProxy] Interpreting ip='*' as all-interfaces. Use 0.0.0.0 or ''.\n",
      "nov 14 15:39:42 kac-srv-001.kac.sblokalnet jupyterhub[58767]: 15:39:42.257 - info: [ConfigProxy] Proxying http://*:8000 to http://127.0.0.1:8081\n",
      "nov 14 15:39:42 kac-srv-001.kac.sblokalnet jupyterhub[58767]: 15:39:42.258 - info: [ConfigProxy] Proxy API at http://127.0.0.1:8001/api/routes\n",
      "nov 14 15:39:42 kac-srv-001.kac.sblokalnet jupyterhub[58767]: [I 2016-11-14 15:39:42.288 JupyterHub app:1254] JupyterHub is now running at http://*:8000/\n",
      "Connection to kac-srv-001.kac.sblokalnet closed.\n"
     ]
    }
   ],
   "source": [
    "remotesudo systemctl status jupyterhub -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to kac-srv-001.kac.sblokalnet closed.\r\n"
     ]
    }
   ],
   "source": [
    "remotesudo systemctl stop jupyterhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "nav_menu": {
    "height": "387px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "1004px",
    "left": "0px",
    "right": "1421px",
    "top": "106px",
    "width": "434px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
